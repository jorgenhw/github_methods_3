---
title: "practical_exercise_3, Methods 3, 2021, autumn semester"
author: 'Jørgen Højlund Wibe'
date: "October 3rd 2021"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries in use
library(pacman)
p_load(tidyverse, lme4, gridExtra)

```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Download and organise the data and model and plot staircase responses based on fits of logistic functions  
2) Fit multilevel models for response times  
3) Fit multilevel models for count data  

This assignment will be part of your final portfolio

## Exercise 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 2 (there should be 29).  
The data is associated with Experiment 2 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

1) Put the data from all subjects into a single data frame  
```{r}
# Loading data
experiment2 <- list.files(path = "data",     # Identify all csv files in folder
                       pattern = "*.csv", full.names = TRUE) %>% 
  lapply(read_csv) %>%                                            # Store all files in list
  bind_rows                                                      # Combine data sets into one data set
```


2) Describe the data and construct extra variables from the existing variables
    i. add a variable to the data frame and call it _correct_ (have it be a _logical_ variable). Assign a 1 to each row where the subject indicated the correct answer and a 0 to each row where the subject indicated the incorrect answer (__Hint:__ the variable _obj.resp_ indicates whether the subject answered "even", _e_ or "odd", _o_, and the variable _target_type_ indicates what was actually presented.

```{r}
experiment2$correct <- (ifelse(experiment2$obj.resp == "e" & experiment2$target.type == "even", "1", 
                              ifelse(experiment2$obj.resp == "o" & experiment2$target.type == "odd", "1",
                                     ifelse(experiment2$obj.resp == "o" & experiment2$target.type == "even", "0",
                                            ifelse(experiment2$obj.resp == "e" & experiment2$target.type == "odd", "0", NA)))))

# Checking if things look as they're supposed to
## table(experiment2$correct)
## table(is.na(experiment2$correct))
experiment2$correct <- as.factor(experiment2$correct)
```


ii. describe what the following variables in the data frame contain, _trial.type_, _pas_, _trial_, _target.contrast_, _cue_, _task_, _target_type_, _rt.subj_, _rt.obj_, _obj.resp_, _subject_ and _correct_. (That means you can ignore the rest of the variables in your description). For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.

**trial.type**
The variable contains two levels and zero NA's; 
level 1: "experiment" (12528 observations)
level 2: "staircase" (5603 observations)
Since it contains two unique levels if makes sense to make them as factors.

'Experiment' describes the actual experimental rounds whereas 'staircase' trials were performed at the beginning of the study, before the researchers collected the actual experimental trials, which maintained a fixed level of contrast throughout.
From WikiP: *"Staircases usually begin with a high intensity stimulus, which is easy to detect. The intensity is then reduced until the observer makes a mistake, at which point the staircase 'reverses' and intensity is increased until the observer responds correctly, triggering another reversal. The values for the last of these 'reversals' are then averaged."*

**pas**
Categorical variable contains four levels and zero NA's.
Level 1 (coded 1): 5627 observations (you didn't see the stimuli)
Level 2 (coded 2): 5176 observations (you saw a weak glimpse)
Level 3 (coded 3): 3370 observations (allmost clear visual)
Level 4 (coded 4): 3958 observations (saw stimuli clearly)

'Pas' refers to the PAS-scale (Perceptual Awareness Scale) which has 4 categorically different ratings: No Experience (NE), Weak Glimpse (WG), Almost Clear Experience (ACE) and Clear Experience (CE).
Since it describes for categorical value, it makes sense to convert the variable to a factor (coded below variable explanations)

**trial**
'trial' numbers each of the trials participants go through.
From a histogram we can see that more participants went through trial 1-250 as opposed to trial 250-431. Also there seems to be a sudden drop in frequency of trials at trial 250.

```{r}
print(
  trial_hist <- ggplot(experiment2, aes(trial)) +
  geom_histogram(colour = "black", fill = "white")
)
```
Each trial contained a number of tasks presented to the participant on a screen.

It's a numerical variable but since it only contains whole numbers it makes sense to convert it into an integer variable.

*target.contrast*
Numeric variable containing the grey-scale proportion of the target digit

The variable contains float values and should thus be kept as numeric.

**cue**
Categorical variable containing the number code for cue ranging from 0-36
One could argue that it should be a factor since there is not an internal hierarchi between the values.

**task**
Categorical value consisting of three levels:
Level 1: pairs (number of obsevations = 6024)
Level 2:  quadruplet (number of obsevations = 6057)
Level 3: singles (number of obsevations = 6050)

**target_type**
Categorical variable with two levels and zero NA's
Level 1: even (9066 observations)
Level 2: odd (9065 observations)

**rt.subj**
Continuous variable
Reaction time (seconds) on the PAS response

**rt.obj**
Continuous variable
Reaction time (seconds) on the target digit

**obj.resp**
Categorical variable with two levels
level 1: e (stands for even)
level 2: 0 (stands for odd)

The key actually pressed e for even and o for odd.
Should be factorized

**subject**
Participant/ID number
Should obviously be factorized.

**correct**
A flag variable telling if response was either correct or incorrect; logical variable.

**Recoding variables**
```{r}
experiment2$trial.type <- as.factor(experiment2$trial.type)
experiment2$target.type <- as.factor(experiment2$target.type)
experiment2$task <- as.factor(experiment2$task)
experiment2$cue <- as.factor(experiment2$cue)
experiment2$trial <- as.integer(experiment2$trial)
experiment2$pas <- as.factor(experiment2$pas)
experiment2$correct <- as.factor(experiment2$correct)
data$obj.resp <- as.factor(data$obj.resp)
data$subject <- as.factor(data$subject)
```

iii. for the staircasing part __only__, create a plot for each subject where you plot the estimated function (on the _target.contrast_ range from 0-1) based on the fitted values of a model (use `glm`) that models _correct_ as dependent on _target.contrast_. These plots will be our _no-pooling_ model. *Comment on the fits - do we have enough data to plot the logistic functions?*

```{r}
# Creating df only containing staircase trials
exp2_staircase <- experiment2 %>% 
  subset(experiment2$trial.type == "staircase")

# Complete pooling model
model1 <- glm(correct ~ target.contrast, data = exp2_staircase, family = binomial(link=logit))
summary(model1)
```

Plotting estimated function on target contrast based on the fitted values of the no pooling model.
```{R}
nopoolfun <- function(subjectID){
  dat <- exp2_staircase[which(exp2_staircase$subject == subjectID),]
  model <- glm(correct ~ target.contrast, family = 'binomial', data=dat)
  fitted <- model$fitted.values
  plot_dat <- data.frame(cbind(fitted,'target.contrast' = dat$target.contrast))
plot <- ggplot(plot_dat, aes(x = target.contrast, y = fitted)) + #fitted
  geom_point()+ 
  geom_line(aes(x = target.contrast, y = fitted))+
  xlab('Target Contrast')+
  ylim(c(0,1))
  theme_minimal()
print(plot)
}

# Running the function for each participant
for (i in unique(exp2_staircase$subject)) {
  nopoolfun(i)
}
```

### Another approach
```{r}
# Model with subject as fixed effect (and thus no pooling)
model1.2 <- glm(correct ~ target.contrast + subject, data = exp2_staircase, family = binomial(link = logit))

exp2_staircase %>%
    ggplot(aes(target.contrast, as.factor(correct), color = correct)) +
    geom_point() +
    geom_line(aes(target.contrast, fitted(model1.2),
        linetype = "fitted values"), inherit.aes = FALSE) +
    scale_linetype_manual(name = "single-level model", values = c("dashed")) +
    facet_wrap(~subject) +
    labs(title = "Correct answers by target contrast per participant",
        subtitle = "With single-level model fitted values (no-pooling)",
        color = "subject correct") +
    ylab("Correct") +
    xlab("target contrast")
```
I have no idea as to why the fitted values are below 0...

iv. on top of those plots, add the estimated functions (on the _target.contrast_ range from 0-1) for each subject based on partial pooling model (use `glmer` from the package `lme4`) where unique intercepts and slopes for _target.contrast_ are modelled for each _subject_ 
```{r}
# Partial pooling model
model2 <- glmer(correct ~ target.contrast + (target.contrast | subject), data = exp2_staircase, family = binomial(link = logit))

#partial
model2 <- glmer(correct~target.contrast + (target.contrast|subject), data = exp2_staircase, family = "binomial")
fitted <- fitted(model2)
exp2_staircase$fitted_values <- fitted
ggplot(exp2_staircase, (aes(x = target.contrast, y = as.numeric(as.character(correct)))))+ 
  geom_point()+
  geom_line(aes(target.contrast, fitted_values), color = "red") +
  facet_wrap(.~subject)+ 
  labs(title = "Partial Pooling", y = "Correct") +
  theme_bw()
```

v. in your own words, describe how the partial pooling model allows for a better fit for each subject

As can be seen from the plot, the partial pooling models takes into consideration the individual differences between the participants as well as general tendencies in the data (hard to tell from plot though). All participants have individual baselines when you do partial pooling which conceptually makes way more sense. 



## Exercise 2

Now we __only__ look at the _experiment_ trials (_trial.type_)  
```{r}
exp2_experiment <- experiment2 %>% 
  subset(experiment2$trial.type == "experiment")
```


1) Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (_rt.obj_) based on a model where only intercept is modelled
    i. comment on these
```{r}
# Only intercept model for participant 1
exp2_exp_ID1 <- exp2_experiment %>% 
  subset(exp2_experiment$subject == "001")
model3 <- lm(rt.obj ~ 1, data = exp2_exp_ID1)
# QQ plot
sub1 <- ggplot(exp2_exp_ID1, aes(sample = rt.obj)) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Q-Q Plot for subject 1") +
theme_bw()
# QQ plot log transformed response time
sub1log <- ggplot(exp2_exp_ID1, aes(sample = log(rt.obj))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles", title = "Q-Q Plot for subject 1", subtitle = "with log transformation")+
theme_bw()

# Only intercept model for participant 2
exp2_exp_ID2 <- exp2_experiment %>% 
  subset(exp2_experiment$subject == "002")
model4 <- lm(rt.obj ~ 1, data = exp2_exp_ID2)
# QQ plot
sub2 <- ggplot(exp2_exp_ID2, aes(sample = rt.obj)) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Q-Q Plot for subject 2") +
theme_bw()
# QQ plot log transformed response time
sub2log <- ggplot(exp2_exp_ID2, aes(sample = log(rt.obj))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles", title = "Q-Q Plot for subject 2", subtitle = "with log transformation")+
theme_bw()

# Only intercept model for participant 3
exp2_exp_ID3 <- exp2_experiment %>% 
  subset(exp2_experiment$subject == "003")
model5 <- lm(rt.obj ~ 1, data = exp2_exp_ID3)
# QQ plot
sub3 <- ggplot(exp2_exp_ID3, aes(sample = rt.obj)) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Q-Q Plot for subject 3") +
theme_bw()
# QQ plot log transformed response time
sub3log <- ggplot(exp2_exp_ID3, aes(sample = log(rt.obj))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles", title = "Q-Q Plot for subject 3", subtitle = "with log transformation")+
theme_bw()

# Only intercept model for participant 4
exp2_exp_ID4 <- exp2_experiment %>% 
  subset(exp2_experiment$subject == "004")
model6 <- lm(rt.obj ~ 1, data = exp2_exp_ID4)
# QQ plot
sub4 <- ggplot(exp2_exp_ID4, aes(sample = rt.obj)) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Q-Q Plot for subject 4") +
theme_bw()
# QQ plot log transformed response time
sub4log <- ggplot(exp2_exp_ID4, aes(sample = log(rt.obj))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles", title = "Q-Q Plot for subject 4", subtitle = "with log transformation")+
theme_bw()


library(gridExtra)
grid.arrange(sub1log, sub2log, sub3log, sub4log, sub1, sub2, sub3, sub4)
```

    ii. does a log-transformation of the response time data improve the Q-Q-plots?
The log transformation removes some of the skewness in the data (which is also one of the purposes of transforming the data). To numerically assess how much influence the log-transformation had on the data, we can run a stat.desc analysis on the non-transformed and transformed data (in this case done for participant 1):
```{r}
round(pastecs::stat.desc(cbind(rt.obj = exp2_exp_ID1$rt.obj, rt.obj_log = log(exp2_exp_ID1$rt.obj)), basic = FALSE, norm = TRUE), digits = 2)
```
To calculate the reduction in skewness, we apply the following formula: (3*(mean-median))/std.dev (source: Larose, D. T., & Larose, C. D. (2014))
```{r}
skewness <- (3*(0.89-0.84))/0.43

skewness_log <- 3*((-0.21)-(-0.18))/0.45

tibble(without_log_trans = skewness[1], with_log_trans = skewness_log)
```
By doing the transformation we went from 0.34 to -0.20 skewness, which is a great deal. We could try other transformations to try to make a better performance, but due to the scope of the assignment we won't do this.


2) Now do a partial pooling model, modeling objective response times as dependent on _task_? (set `REML=FALSE` in your `lmer`-specification)
```{r}
# Partial pooling, with RT dep on task
model7 <- lmer(rt.obj ~ task + (1| subject), data = exp2_experiment, REML = FALSE)
```

    i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modelling)

First of, I would include a random intercept for subject since each subject has their own baseline. Also multiple measurements per participant violates assumption of independence for normal linear regression and as a consequence causes type 1 errors.

Random slope: We expect different effects for different people
Random intercept: We expect different baselines
```{r}
# Just random intercept
model8 <- lmer(rt.obj ~ task + (1| subject), data = exp2_experiment, REML = F)

# Random intercept and random slope (singular: overfitting the data)
model9 <- lmer(rt.obj ~ task + (1 + pas| subject), data = exp2_experiment, REML = F)

# Random intercept and random slope (singular: overfitting the data)
model10 <- lmer(rt.obj ~ task + (1| subject), data = exp2_experiment, REML = F)

# Two random intercepts
model12 <- lmer(rt.obj ~ task + (1 | subject) + (1 | pas), data = exp2_experiment, REML = F)

# Three random intercepts
model14 <- lmer(rt.obj ~ task + (1 | subject) + (1 | trial) + (1 | task), data = exp2_experiment, REML = F)


summary(model8)
MuMIn::r.squaredGLMM(model8) #R2c = 0.01464871

MuMIn::r.squaredGLMM(model13) #R2c = 0.01464871
```

    ii. explain in your own words what your chosen models says about response times between the different tasks  
We see, that response time is fastest in the pairs condition (1.12 seconds). When going to the quadruplet condition, reaction time decreases with 0.15 seconds and when going to singles task, RT decreases with 0.19 seconds. From the lmer we can't tell if this is a significant effect. In terms of variance explained, the model is pretty shitty with an r-squared of 0.014.


3) Now add _pas_ and its interaction with _task_ to the fixed effects  
```{R}
pas_interaction_model <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial), REML=FALSE, data = experiment2)
summary(pas_interaction_model)
```

  i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits?
```{R}
converge1 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|cue), REML=FALSE, data = experiment2)

converge2 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|cue) + (1|target.contrast), REML=FALSE, data = experiment2)

converge3 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|cue) + (1|target.contrast) + (1|rt.subj), REML=FALSE, data = experiment2)
```

After adding the fifth random intercept, the model would not converge anymore.

  ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)
```{r}
isSin1 <- lmerTest::lmer(rt.obj ~ task + pas + task:pas + (1|subject) + (1|trial) + (1|rt.subj) + (1|even.digit) + (1|cue), REML=FALSE, data = experiment2)
```

```{r}
print(VarCorr(isSin1), comp='Variance')
```

The reason why the model is singular is because the differnt random intecepts leads to random-effect estimates of almost zero (as can be seen with even.digit). 



  iii. in your own words - how could you explain why your model would result in a singular fit?

Fitting overly complex models, such that the variance-covariance matrices cannot be estimated precisely enough results in a singular fit. In other words: Singular fits is also equivalent to saying that your model is overfitting the data.

## Exercise 3

1) Initialise a new data frame, `data.count`. _count_ should indicate the number of times they categorized their experience as _pas_ 1-4 for each _task_. I.e. the data frame would have for subject 1: for task:singles, pas1 was used # times, pas2 was used # times, pas3 was used # times and pas4 was used # times. You would then do the same for task:pairs and task:quadruplet  

```{r}
countData <- experiment2 %>% 
  group_by(subject, task, pas) %>% 
  summarise("count" = n())
```        

2) Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled
```{r}
pasModel <- glmer(count ~ pas*task + (pas|subject), data = countData, family = poisson, control = glmerControl(optimizer="bobyqa"))
summary(pasModel)
```

    i. which family should be used?  
Since we are dealing with count data, poission family should be used.

    ii. why is a slope for _pas_ not really being modelled?  

    iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)

    iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction  
```{r}
pasModelNoInteraction <- glmer(count ~ pas + task + (pas|subject), data = countData, family = poisson, control = glmerControl(optimizer="bobyqa"))
```

Comparison of models
```{r}

```


    v. indicate which of the two models, you would choose and why  

    vi. based on your chosen model - write a short report on what this says about the distribution of ratings as dependent on _pas_ and _task_  

    vii. include a plot that shows the estimated amount of ratings for four subjects of your choosing 
3) Finally, fit a multilevel model that models _correct_ as dependent on _task_ with a unique intercept for each _subject_  
    i. does _task_ explain performance?  
    ii. add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
    iii. now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
    iv. finally, fit a model that models the interaction between _task_ and _pas_  and their main effects  
    v. describe in your words which model is the best in explaining the variance in accuracy  

